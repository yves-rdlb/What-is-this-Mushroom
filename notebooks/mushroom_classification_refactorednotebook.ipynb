{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RikIANObmJRo"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import gc\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"zlatan599/mushroom1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, mixed_precision\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B0, EfficientNetV2B1\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as efficientnet_preprocess\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TensorBoard\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n"
      ],
      "metadata": {
        "id": "ExeFRSlb7IKI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#speed up training and reduce memory usage\n",
        "\n",
        "mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "rEv1KZIts6rW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "LABEL_SMOOTHING = 0.1\n",
        "EPOCHS_HEAD = 10\n",
        "EPOCHS_FINE = 15"
      ],
      "metadata": {
        "id": "2m-D-nD_s6uW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load and clean and prepare data"
      ],
      "metadata": {
        "id": "CK_tm6bntI3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download data from kaggle"
      ],
      "metadata": {
        "id": "DKEurjywtc9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"zlatan599/mushroom1\")\n",
        "BASE_PATH = \"/kaggle/input/mushroom1/merged_dataset\""
      ],
      "metadata": {
        "id": "wJP93mbptZls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e7ab27-657e-46e1-a0c7-082b79ddc1a5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'mushroom1' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data():\n",
        "  train = pd.read_csv('/kaggle/input/mushroom1/train.csv')\n",
        "  test = pd.read_csv('/kaggle/input/mushroom1/test.csv')\n",
        "  val = pd.read_csv('/kaggle/input/mushroom1/val.csv')\n",
        "  # Replace Kaggle path with Colab base path\n",
        "  for df in [train, test, val]:\n",
        "    df[\"image_path\"] = df[\"image_path\"].str.replace(\"/kaggle/working/merged_dataset\", BASE_PATH)\n",
        "    #drop duplicates\n",
        "    df.drop_duplicates(subset=\"image_path\", inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "  return train, val, test\n",
        "\n",
        "train_df, val_df, test_df = load_and_prepare_data()"
      ],
      "metadata": {
        "id": "gDkqvC9rs6w1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert string labels into integer indices"
      ],
      "metadata": {
        "id": "21jieFvtuaL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_lookup = tf.keras.layers.StringLookup(\n",
        "    vocabulary=sorted(train_df['label'].unique()),\n",
        "    num_oov_indices=0,\n",
        "    output_mode=\"int\")\n",
        "\n",
        "NUM_CLASSES = len(label_lookup.get_vocabulary())"
      ],
      "metadata": {
        "id": "IDXccPkHs6ze"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  data preparation pipeline construction\n"
      ],
      "metadata": {
        "id": "YLjH_3xpu6Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale01(img):\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    return img / 255.0"
      ],
      "metadata": {
        "id": "iL5ogXaM-SDA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tf_dataset(df, preprocess_fn=rescale01, augment=False, heavy=False, shuffle=False):\n",
        "  def decode(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3) # decode the JPEG image into a tensor with 3 channels\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    #image = preprocess_input(image)\n",
        "    image = preprocess_fn(image)    # apply model-specific preprocessing\n",
        "    label = tf.one_hot(label_lookup(label), depth=NUM_CLASSES) # convert the string label to an integer using label_lookup, then one-hot encodes it\n",
        "    return image, label\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices((df[\"image_path\"].values, df[\"label\"].values)) # create a TensorFlow dataset\n",
        "  ds = ds.map(decode, num_parallel_calls=AUTOTUNE)\n",
        "  if augment:\n",
        "      if not heavy:\n",
        "          aug_layer = tf.keras.Sequential([\n",
        "              layers.RandomFlip(\"horizontal\"),\n",
        "              layers.RandomRotation(0.2),\n",
        "              layers.RandomZoom(0.2),\n",
        "              layers.RandomContrast(0.2),\n",
        "          ])\n",
        "      else:\n",
        "          aug_layer = tf.keras.Sequential([\n",
        "              layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "              layers.RandomRotation(0.3),\n",
        "              layers.RandomZoom(0.3),\n",
        "              layers.RandomContrast(0.4),\n",
        "              layers.RandomBrightness(0.3),\n",
        "          ])\n",
        "      ds = ds.map(lambda x, y: (aug_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  if shuffle:\n",
        "      ds = ds.shuffle(1024)\n",
        "  return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE) #Batch the dataset into groups of BATCH_SIZE + prefetch batches in the background to improve training performance"
      ],
      "metadata": {
        "id": "yA_aXkaGs61o"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# function to save the model name"
      ],
      "metadata": {
        "id": "-8PKacdsy2tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model_name(config):\n",
        "    name = f\"{config['arch']}_{IMG_SIZE[0]}px\"\n",
        "    name += \"_heavyaug\" if config.get(\"heavyaug\") else \"_aug\" if config.get(\"augment\") else \"_noaug\"\n",
        "    name += f\"_finetune{config['unfreeze']}\" if config.get(\"finetune\") else \"_nofinetune\"\n",
        "    name += f\"_{config['epochs']}+{EPOCHS_FINE}ep\" if config.get(\"finetune\") else f\"_{config['epochs']}ep\"\n",
        "    if config.get(\"label_smoothing\"):\n",
        "        name += f\"_ls{config['label_smoothing']}\"\n",
        "    return name\n"
      ],
      "metadata": {
        "id": "GM6Rw6tWs64w"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vzCjdAWy1UE"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train and fine-tune pipeline"
      ],
      "metadata": {
        "id": "85clc9OtztEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_results = []"
      ],
      "metadata": {
        "id": "AAILuiG0tKZ6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_finetune(config):\n",
        "\n",
        "  preprocess_fn = config.get(\"preprocess\", rescale01)\n",
        "\n",
        "  #  build datasets\n",
        "  #train_ds = build_tf_dataset(train_df, preprocess_fn, augment=config.get(\"augment\"), heavy=config.get(\"heavyaug\"), shuffle=True)\n",
        "  train_ds = build_tf_dataset(df=train_df, preprocess_fn=preprocess_fn, augment=config.get(\"augment\"), heavy=config.get(\"heavyaug\"), shuffle=True)\n",
        "  val_ds = build_tf_dataset(val_df, preprocess_fn)\n",
        "  test_ds = build_tf_dataset(test_df, preprocess_fn)\n",
        "\n",
        "  #  Model  construction\n",
        "  base = config['backbone'](include_top=False, input_shape=(*IMG_SIZE, 3), weights='imagenet') # Load a pretrained model\n",
        "  base.trainable = False # Freeze the backbone model so its weights aren‚Äôt updated during initial training\n",
        "\n",
        "  model = models.Sequential([\n",
        "    base,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax', dtype='float32')\n",
        "    ])\n",
        "\n",
        "  loss = CategoricalCrossentropy(label_smoothing=config.get(\"label_smoothing\", 0))\n",
        "  name = generate_model_name(config)\n",
        "\n",
        "  #  Compile & Callbacks\n",
        "  ##      set up a dynamic learning rate schedule using cosine decay with restarts\n",
        "  ###     calculate how many batches make up one epoch\n",
        "  # steps_per_epoch = len(train_df) // BATCH_SIZE\n",
        "  steps_per_epoch = max(1, len(train_df) // BATCH_SIZE)\n",
        "  lr1 = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
        "      1e-3, 5 * steps_per_epoch,\n",
        "      t_mul=2.0,\n",
        "      m_mul=0.8) # create a learning rate schedule that: Starts at a high learning rate (1e-3) Gradually decays it using a cosine curve Then restarts the cycle periodically\n",
        "  model.compile(optimizer=Adam(learning_rate=lr1), loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  ## Sets up callbacks for Training\n",
        "  logdir = f\"logs/{name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "  callbacks = [\n",
        "    EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint(f\"{name}.keras\", save_best_only=True, monitor=\"val_accuracy\"),\n",
        "    CSVLogger(f\"{name}.csv\"),\n",
        "    TensorBoard(log_dir=logdir)\n",
        "    ]\n",
        "  # start timing\n",
        "  start = time.time()\n",
        "  #  Train Head\n",
        "  print(f\"\\nüöÄ Training: {name}\")\n",
        "  model.fit(train_ds, validation_data=val_ds, epochs=config['epochs'], callbacks=callbacks, verbose=1)\n",
        "\n",
        "  # Fine-tune\n",
        "  if config.get(\"finetune\"):\n",
        "    unfreeze_from = int(len(base.layers) * config['unfreeze'])\n",
        "    for i, layer in enumerate(base.layers):\n",
        "      layer.trainable = (i >= unfreeze_from and not isinstance(layer, layers.BatchNormalization))\n",
        "\n",
        "\n",
        "    lr2 = tf.keras.optimizers.schedules.CosineDecayRestarts(3e-5, 8 * steps_per_epoch, t_mul=2.0, m_mul=0.9)\n",
        "    model.compile(optimizer=AdamW(learning_rate=lr2, weight_decay=5e-5), loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    print(f\"üîì Fine-tuning from layer {unfreeze_from}/{len(base.layers)}\")\n",
        "    history_finetune = model.fit(train_ds, validation_data=val_ds,\n",
        "                                     epochs=EPOCHS_FINE, callbacks=callbacks, verbose=1)\n",
        "  #  Evaluate\n",
        "  print(f\"\\n‚úÖ Final evaluation: {name}\")\n",
        "\n",
        "  test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "  end = time.time()\n",
        "  # Save final model explicitly\n",
        "  model.save(f\"{name}_final.keras\")\n",
        "\n",
        "  #log results\n",
        "  all_model_results.append({\n",
        "        \"model_name\": name,\n",
        "        \"arch\": config['arch'],\n",
        "        \"finetune\": config.get(\"finetune\", False),\n",
        "        \"unfreeze\": config.get(\"unfreeze\", 0.0) if config.get(\"finetune\") else 0.0,\n",
        "        \"heavyaug\": config.get(\"heavyaug\", False),\n",
        "        \"label_smoothing\": config.get(\"label_smoothing\", 0.0),\n",
        "        \"head_epochs\": config[\"epochs\"],\n",
        "        \"finetune_epochs\": EPOCHS_FINE if config.get(\"finetune\") else 0,\n",
        "        \"total_time_minutes\": round((end - start)/60, 2),\n",
        "        \"test_accuracy\": round(float(test_acc), 4),\n",
        "        \"test_loss\": round(float(test_loss), 4),\n",
        "        \"trainable_params\": int(np.sum([np.prod(v.shape) for v in model.trainable_variables])),\n",
        "        \"total_params\": int(model.count_params())\n",
        "    })\n",
        "\n",
        "     # Clear memory\n",
        "  tf.keras.backend.clear_session()\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "tvxYLEbxy1Wr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train different configs"
      ],
      "metadata": {
        "id": "FuiuO86U0lO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Configurations to Train\n",
        "configs = [\n",
        "    # {\"arch\": \"baselinecnn\",\n",
        "    #  \"backbone\": lambda **kwargs: models.Sequential([\n",
        "    #      layers.Input(shape=(224,224,3)),\n",
        "    #      layers.Conv2D(32,3,activation='relu'),\n",
        "    #      layers.MaxPooling2D(),\n",
        "    #      layers.Conv2D(64,3,activation='relu'),\n",
        "    #      layers.MaxPooling2D()\n",
        "    #  ]),\n",
        "    #  \"preprocess\": rescale01,\n",
        "    #  \"epochs\": 10},\n",
        "\n",
        "    # {\"arch\": \"mobilenetv2\",\n",
        "    #  \"backbone\": MobileNetV2,\n",
        "    #  \"preprocess\": mobilenet_preprocess,\n",
        "    #  \"epochs\": 8,\n",
        "    #  \"augment\": True},\n",
        "\n",
        "    # {\"arch\": \"mobilenetv2\",\n",
        "    #  \"backbone\": MobileNetV2,\n",
        "    #  \"preprocess\": mobilenet_preprocess,\n",
        "    #  \"epochs\": 8,\n",
        "    #  \"augment\": True,\n",
        "    #  \"finetune\": True,\n",
        "    #  \"unfreeze\": 0.3},\n",
        "\n",
        "    # {\"arch\": \"efficientnetv2b0\",\n",
        "    #  \"backbone\": EfficientNetV2B0,\n",
        "    #  \"preprocess\": efficientnet_preprocess,\n",
        "    #  \"epochs\": 10,\n",
        "    #  \"augment\": True,\n",
        "    #  \"finetune\": True,\n",
        "    #  \"unfreeze\": 0.3},\n",
        "    {\n",
        "    \"arch\": \"densenet201\",\n",
        "    \"backbone\": DenseNet201,\n",
        "    \"preprocess\": densenet_preprocess,\n",
        "    \"epochs\": 10,\n",
        "    \"augment\": True,\n",
        "    \"finetune\": True,\n",
        "    \"unfreeze\": 0.4,\n",
        "    \"label_smoothing\": 0.1\n",
        "    },\n",
        "    {\"arch\": \"efficientnetv2b0\",\n",
        "     \"backbone\": EfficientNetV2B0,\n",
        "     \"preprocess\": efficientnet_preprocess,\n",
        "     \"epochs\": 10,\n",
        "     \"augment\": True,\n",
        "     \"heavyaug\": True,\n",
        "     \"finetune\": True,\n",
        "     \"unfreeze\": 0.3,\n",
        "     \"label_smoothing\": 0.1},\n",
        "\n",
        "    {\"arch\": \"efficientnetv2b1\",\n",
        "     \"backbone\": EfficientNetV2B1,\n",
        "     \"preprocess\": efficientnet_preprocess,\n",
        "     \"epochs\": 12,\n",
        "     \"augment\": True,\n",
        "     \"finetune\": True,\n",
        "     \"unfreeze\": 0.5,\n",
        "     \"label_smoothing\": 0.15}\n",
        "]\n"
      ],
      "metadata": {
        "id": "TqMUupjXy1Yv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WLx1NSpd9pbh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for config in configs:\n",
        "  train_and_finetune(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogAzK4oby1d1",
        "outputId": "0b4bc9fb-811f-4517-8397-e49d841b0e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m74836368/74836368\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\n",
            "üöÄ Training: densenet201_224px_aug_finetune0.4_10+15ep_ls0.1\n",
            "Epoch 1/10\n",
            "\u001b[1m  20/2277\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m8:28:30\u001b[0m 14s/step - accuracy: 0.0625 - loss: 5.1048"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Export Summary ---\n",
        "summary_df = pd.DataFrame(all_model_results)\n",
        "summary_df.to_csv(\"model_summary.csv\", index=False)\n",
        "display(summary_df.sort_values(\"test_accuracy\", ascending=False))"
      ],
      "metadata": {
        "id": "Rthk4oMatDQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n"
      ],
      "metadata": {
        "id": "eZhjwSOSl_fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfOpG6Eol_iJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}